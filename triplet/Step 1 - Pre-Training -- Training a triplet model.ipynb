{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to train the triplet network (or feature extractor). During the fine-tuning phase, this feature extractor is used to extract the features of the power trace amd train appropriate machine learning classifier (in this study k-NN classifier).\n",
    "\n",
    "To train the triplet model, make the changes listed under \"Triplet Model Parameters\" in this notebook. More specifically, changes should be made to the dictionary \"data params.\" The following is a description of each dictionary parameter:\n",
    "\n",
    "- ~~input_path: The directory containing the dataset for training the triplet model is specified by input path.~~\n",
    "- input path: The file containing the dataset for training the triplet model is specified by input path.\n",
    "- target_byte: The attack byte is designated as target byte.\n",
    "- start_idx: starting index of the attack window.\n",
    "- end_idx: last index of the attack window.\n",
    "- testType: The testType parameter is used to select the appropriate dataset with the same or different key as the training dataset. It is always \"samekey\" when training the triplet model.\n",
    "- n: Number of samples per class utilized for training the triplet network.\n",
    "- triplet_model_path: Path to save the trained triplet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 590 µs (2022-03-10T15:25:23/2022-03-10T15:25:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append('../utilities/')\n",
    "\n",
    "from modelTrainingUtility import *\n",
    "from modelZoo import triplet_cnn\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Lambda, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters of the triplet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 745 µs (2022-03-10T15:25:23/2022-03-10T15:25:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters for loading the dataset for training the model\n",
    "data_params = {\n",
    "    \"input_path\": '../../TripletPower-data/stm32f-unmasked/PC2_CB2_TDS3_K0_U_200k/train_same_key.npz',  # path to load the dataset\n",
    "    \"target_byte\": 2,  # byte for which ranking is to be performed\n",
    "    \"start_idx\": 1200,\n",
    "    \"end_idx\": 2200,\n",
    "    \"testType\": \"samekey\",\n",
    "    \"n\": 100,  # number of samples to be selected from each class for training feature extractor\n",
    "    \"triplet_model_path\": '../models/feature-extractor-model/stm32f-unmasked/' # path to save the trained triplet model (or feature extractor)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 702 µs (2022-03-10T15:25:23/2022-03-10T15:25:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_pos_pairs_for_id(classid):  # classid --> e.g. 0\n",
    "    traces = classes_to_ids[classid]\n",
    "    # pos_pairs is actually the combination C(10,2)\n",
    "    # e.g. if we have 10 example [0,1,2,...,9]\n",
    "    # and want to create a pair [a, b], where (a, b) are different and order does not matter\n",
    "    # e.g. [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9),\n",
    "    # (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9)...]\n",
    "    # C(10, 2) = 45\n",
    "    pos_pairs = [(traces[i], traces[j]) for i in range(len(traces)) for j in range(i + 1, len(traces))]\n",
    "    random.shuffle(pos_pairs)\n",
    "    return pos_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 847 µs (2022-03-10T15:25:24/2022-03-10T15:25:24)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_positive_pairs(class_id_range):\n",
    "    # class_id_range = range(0, num_classes)\n",
    "    listX1 = []\n",
    "    listX2 = []\n",
    "    for class_id in class_id_range:\n",
    "        pos = build_pos_pairs_for_id(class_id)\n",
    "        # -- pos [(1, 9), (0, 9), (3, 9), (4, 8), (1, 4),...] --> (anchor example, positive example)\n",
    "        for pair in pos:\n",
    "            listX1 += [pair[0]]  # identity\n",
    "            listX2 += [pair[1]]  # positive example\n",
    "    perm = np.random.permutation(len(listX1))\n",
    "    # random.permutation([1,2,3]) --> [2,1,3] just random\n",
    "    # random.permutation(5) --> [1,0,4,3,2]\n",
    "    # In this case, we just create the random index\n",
    "    # Then return pairs of (identity, positive example)\n",
    "    # that each element in pairs in term of its index is randomly ordered.\n",
    "    return np.array(listX1)[perm], np.array(listX2)[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 973 µs (2022-03-10T15:25:24/2022-03-10T15:25:24)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a loss which doesn't take into account the y_true, as# Build\n",
    "# we'll be passing only 0\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "# The triplet loss\n",
    "def cosine_triplet_loss(X):\n",
    "    _alpha = alpha_value\n",
    "    positive_sim, negative_sim = X\n",
    "\n",
    "    losses = K.maximum(0.0, negative_sim - positive_sim + _alpha)\n",
    "    return K.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.85 ms (2022-03-10T15:25:24/2022-03-10T15:25:24)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------- Hard Triplet Mining -----------\n",
    "# Naive way to compute all similarities between all network traces.\n",
    "\n",
    "def build_similarities(conv, all_imgs):\n",
    "    embs = conv.predict(all_imgs)\n",
    "    embs = embs / np.linalg.norm(embs, axis=-1, keepdims=True)\n",
    "    all_sims = np.dot(embs, embs.T)\n",
    "    return all_sims\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def build_negatives(anc_idxs, pos_idxs, similarities, neg_imgs_idx, num_retries=50):\n",
    "    # If no similarities were computed, return a random negative\n",
    "    if similarities is None:\n",
    "        return random.sample(neg_imgs_idx,len(anc_idxs))\n",
    "    final_neg = []\n",
    "    # for each positive pair\n",
    "    for (anc_idx, pos_idx) in zip(anc_idxs, pos_idxs):\n",
    "        anchor_class = id_to_classid[anc_idx]\n",
    "        #positive similarity\n",
    "        sim = similarities[anc_idx, pos_idx]\n",
    "        # find all negatives which are semi(hard)\n",
    "        possible_ids = np.where((similarities[anc_idx] + alpha_value) > sim)[0]\n",
    "        possible_ids = intersect(neg_imgs_idx, possible_ids)\n",
    "        appended = False\n",
    "        for iteration in range(num_retries):\n",
    "            if len(possible_ids) == 0:\n",
    "                break\n",
    "            idx_neg = random.choice(possible_ids)\n",
    "            if id_to_classid[idx_neg] != anchor_class:\n",
    "                final_neg.append(idx_neg)\n",
    "                appended = True\n",
    "                break\n",
    "        if not appended:\n",
    "            final_neg.append(random.choice(neg_imgs_idx))\n",
    "    return final_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.86 ms (2022-03-10T15:25:24/2022-03-10T15:25:24)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SemiHardTripletGenerator():\n",
    "    def __init__(self, Xa_train, Xp_train, batch_size, all_traces, neg_traces_idx, conv):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.traces = all_traces\n",
    "        self.Xa = Xa_train\n",
    "        self.Xp = Xp_train\n",
    "        self.cur_train_index = 0\n",
    "        self.num_samples = Xa_train.shape[0]\n",
    "        self.neg_traces_idx = neg_traces_idx\n",
    "        self.all_anchors = list(set(Xa_train))\n",
    "        self.mapping_pos = {v: k for k, v in enumerate(self.all_anchors)}\n",
    "        self.mapping_neg = {k: v for k, v in enumerate(self.neg_traces_idx)}\n",
    "        if conv:\n",
    "            self.similarities = build_similarities(conv, self.traces)\n",
    "        else:\n",
    "            self.similarities = None\n",
    "\n",
    "    def next_train(self):\n",
    "        while 1:\n",
    "            self.cur_train_index += self.batch_size\n",
    "            if self.cur_train_index >= self.num_samples:\n",
    "                self.cur_train_index = 0\n",
    "\n",
    "            # fill one batch\n",
    "            traces_a = self.Xa[self.cur_train_index:self.cur_train_index + self.batch_size]\n",
    "            traces_p = self.Xp[self.cur_train_index:self.cur_train_index + self.batch_size]\n",
    "            traces_n = build_negatives(traces_a, traces_p, self.similarities, self.neg_traces_idx)\n",
    "\n",
    "            yield ([self.traces[traces_a],\n",
    "                    self.traces[traces_p],\n",
    "                    self.traces[traces_n]],\n",
    "                   np.zeros(shape=(traces_a.shape[0]))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 48.6 s (2022-03-10T15:25:24/2022-03-10T15:26:13)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for training model ...\n",
      "processing data for key byte 2\n",
      "training data loaded successfully!\n",
      "shape of X_profiling:  (200000, 1000)\n",
      "shape of y_profiling:  (200000,)\n",
      "key used in pre-training phase:  [ 43 126  21  22  40 174 210 166 171 247  21 136   9 207  79  60]\n",
      "number of classes in the dataset:  256\n",
      "shape of the power traces to be used for training:  (25600, 1000)\n",
      "reshaped of the dataset for training:  (25600, 1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load the power traces on which base model is to be trained (Pre-training phase)\n",
    "X_profiling, y_profiling, key = load_training_data_2(data_params)\n",
    "# shape of the dataset\n",
    "print('shape of X_profiling: ', X_profiling.shape)\n",
    "print('shape of y_profiling: ', y_profiling.shape)\n",
    "print('key used in pre-training phase: ', key)\n",
    "\n",
    "# number of unique classes in the dataset\n",
    "nb_classes = len(np.unique(y_profiling))\n",
    "print('number of classes in the dataset: ', nb_classes)\n",
    "\n",
    "# getting the subset of the dataset\n",
    "x, y, all_data_df = create_df(X_profiling, y_profiling, data_params['n'])\n",
    "\n",
    "# reshaping the dataset for training\n",
    "all_traces = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "print('reshaped of the dataset for training: ', all_traces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 4.84 s (2022-03-10T15:26:13/2022-03-10T15:26:18)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train Anchor:  (1267200,)\n",
      "shape of X_train Positive:  (1267200,)\n",
      "with parameters, Alpha: 0.1, Batch_size: 100, Embedded_size: 512, Epoch_num: 10, N: 100\n",
      "Triplet_Model_emb_size_512_epochs_10_target_byte_2_nsamples_100\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor (InputLayer)             [(None, 1000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive (InputLayer)           [(None, 1000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative (InputLayer)           [(None, 1000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, 512)          8710784     anchor[0][0]                     \n",
      "                                                                 positive[0][0]                   \n",
      "                                                                 negative[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           functional_1[0][0]               \n",
      "                                                                 functional_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           functional_1[0][0]               \n",
      "                                                                 functional_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 ()                   0           dot[0][0]                        \n",
      "                                                                 dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 8,710,784\n",
      "Trainable params: 8,710,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create groups for all the classes\n",
    "all_data_group = all_data_df.groupby(['label'])\n",
    "# build mapping between classes and power traces\n",
    "classes_to_ids = all_data_group.groups\n",
    "# classes_to_ids --> {0: [0, 1, 2, 3, 4, 5], 1: [1, 2, 3, 4, 5]}\n",
    "# print(classes_to_ids)\n",
    "id_to_classid = {v: c for c, traces in classes_to_ids.items() for v in traces}\n",
    "# id_to_classid --> {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 1,...]\n",
    "# print(id_to_classid)\n",
    "\n",
    "# generating anchor and positive pairs\n",
    "Xa_train, Xp_train = build_positive_pairs(range(0, nb_classes))\n",
    "\n",
    "# Gather the ids of all power traces that are used for training\n",
    "# This just union of two sets set(A) | set(B)\n",
    "all_traces_train_idx = list(set(Xa_train) | set(Xp_train))\n",
    "print(\"shape of X_train Anchor: \", Xa_train.shape)\n",
    "print(\"shape of X_train Positive: \", Xp_train.shape)\n",
    "\n",
    "# parameters for training the triplet network\n",
    "alpha = 0.1 # margin\n",
    "batch_size_value = 100\n",
    "emb_size = 512\n",
    "number_epoch = 10 # default was 10 \n",
    "opt = RMSprop(lr=0.00001)\n",
    "\n",
    "description = 'Triplet_Model' + '_emb_size_' + str(emb_size) + '_epochs_' + str(number_epoch) + '_target_byte_' + str(data_params[\"target_byte\"]) + '_nsamples_' + str(data_params[\"n\"])\n",
    "print(\"with parameters, Alpha: %s, Batch_size: %s, Embedded_size: %s, Epoch_num: %s, N: %s\" %\n",
    "      (alpha, batch_size_value, emb_size, number_epoch, data_params[\"n\"]))\n",
    "\n",
    "alpha_value = float(alpha)\n",
    "print(description)\n",
    "\n",
    "# path to save triplet model and its statistics\n",
    "model_dir_path = data_params[\"triplet_model_path\"]\n",
    "if not os.path.isdir(model_dir_path):\n",
    "    os.makedirs(model_dir_path)\n",
    "\n",
    "model_dir = os.path.join(model_dir_path)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "shared_conv2 = triplet_cnn(input_shape=(all_traces.shape[1], 1), emb_size=emb_size)\n",
    "\n",
    "anchor = Input((all_traces.shape[1], 1), name='anchor')\n",
    "positive = Input((all_traces.shape[1], 1), name='positive')\n",
    "negative = Input((all_traces.shape[1], 1), name='negative')\n",
    "\n",
    "a = shared_conv2(anchor)\n",
    "p = shared_conv2(positive)\n",
    "n = shared_conv2(negative)\n",
    "\n",
    "# The Dot layer in Keras now supports built-in Cosine similarity using the normalize = True parameter.\n",
    "# From the Keras Docs:\n",
    "# keras.layers.Dot(axes, normalize=True)\n",
    "# normalize: Whether to L2-normalize samples along the dot product axis before taking the dot product.\n",
    "#  If set to True, then the output of the dot product is the cosine proximity between the two samples.\n",
    "pos_sim = Dot(axes=-1, normalize=True)([a, p])\n",
    "neg_sim = Dot(axes=-1, normalize=True)([a, n])\n",
    "\n",
    "# customized loss\n",
    "loss = Lambda(cosine_triplet_loss,\n",
    "              output_shape=(1,))(\n",
    "    [pos_sim, neg_sim])\n",
    "\n",
    "model_triplet = Model(\n",
    "    inputs=[anchor, positive, negative],\n",
    "    outputs=loss)\n",
    "\n",
    "print(model_triplet.summary())\n",
    "\n",
    "# compiling the triplet model\n",
    "model_triplet.compile(loss=identity_loss, optimizer=opt)\n",
    "# batch size\n",
    "batch_size = batch_size_value\n",
    "# At first epoch we don't generate hard triplets\n",
    "gen_hard = SemiHardTripletGenerator(Xa_train, Xp_train, batch_size, all_traces, all_traces_train_idx, None)\n",
    "nb_epochs = number_epoch\n",
    "csv_logger = CSVLogger(model_dir + 'Training_Log_%s.csv' % description, append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 3 h 15 min 5 s (2022-03-10T15:26:18/2022-03-10T18:41:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a feature extractor ...\n",
      "built new hard generator for epoch 0\n",
      "WARNING:tensorflow:From <ipython-input-10-b626c599e8a4>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "    2/12672 [..............................] - ETA: 6:19 - loss: 0.1000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0219s vs `on_train_batch_end` time: 0.0373s). Check your callbacks.\n",
      "12672/12672 [==============================] - 616s 49ms/step - loss: 0.0153\n",
      "built new hard generator for epoch 1\n",
      "12672/12672 [==============================] - 1527s 121ms/step - loss: 0.0422\n",
      "built new hard generator for epoch 2\n",
      "12672/12672 [==============================] - 1388s 110ms/step - loss: 0.0435\n",
      "built new hard generator for epoch 3\n",
      "12672/12672 [==============================] - 1229s 97ms/step - loss: 0.0497 - ETA: 6s - l -  - ETA: 0s - loss: 0\n",
      "built new hard generator for epoch 4\n",
      "12672/12672 [==============================] - 1173s 93ms/step - loss: 0.0548\n",
      "built new hard generator for epoch 5\n",
      "12672/12672 [==============================] - 1160s 92ms/step - loss: 0.0558\n",
      "built new hard generator for epoch 6\n",
      "12672/12672 [==============================] - 1153s 91ms/step - loss: 0.0551s - loss:\n",
      "built new hard generator for epoch 7\n",
      "12672/12672 [==============================] - 1128s 89ms/step - loss: 0.0573s - loss: 0. - ETA: 4s - ETA: 3s - loss: 0.05 - E - ETA: 1s\n",
      "built new hard generator for epoch 8\n",
      "    2/12672 [..............................] - ETA: 10:44 - loss: 0.0689WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0240s vs `on_train_batch_end` time: 0.0683s). Check your callbacks.\n",
      "12672/12672 [==============================] - 1129s 89ms/step - loss: 0.0552s - loss\n",
      "built new hard generator for epoch 9\n",
      "    2/12672 [..............................] - ETA: 9:26 - loss: 0.0670WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0214s vs `on_train_batch_end` time: 0.0664s). Check your callbacks.\n",
      "12672/12672 [==============================] - 1124s 89ms/step - loss: 0.0550s - loss: 0.055 - ETA: 7s - lo\n",
      "feature extractor trained and saved successfully at ../models/feature-extractor-model/stm32f-unmasked/\n"
     ]
    }
   ],
   "source": [
    "print('Training a feature extractor ...')\n",
    "for epoch in range(nb_epochs):\n",
    "    print(\"built new hard generator for epoch \" + str(epoch))\n",
    "    model_triplet.fit_generator(generator=gen_hard.next_train(),\n",
    "                                steps_per_epoch=Xa_train.shape[0] // batch_size,\n",
    "                                epochs=1, verbose=1, callbacks=[csv_logger])\n",
    "    gen_hard = SemiHardTripletGenerator(Xa_train, Xp_train, batch_size, all_traces, all_traces_train_idx,\n",
    "                                        shared_conv2)\n",
    "    # For no semi-hard_triplet\n",
    "    # gen_hard = HardTripletGenerator(Xa_train, Xp_train, batch_size, all_traces, all_traces_train_idx, None)\n",
    "    \n",
    "shared_conv2.save(model_dir + '%s.hdf5' % description)\n",
    "print('feature extractor trained and saved successfully at %s' % model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
